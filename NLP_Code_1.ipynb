{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8473b191-45c5-42d6-8aea-3960df473910",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d721f427-0b3a-413e-b708-0bff32180d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "string1 = ['Hi how are you. I am well.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "612d2db8-8b4e-4157-abd4-c966551f5931",
   "metadata": {},
   "outputs": [],
   "source": [
    "string2 = 'Hi how are you. I am well.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "834cf485-41f8-4168-b351-ddd9922e4b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "countvectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a9183e6e-9e1c-47bd-9000-cf4f720f73d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = countvectorizer.fit_transform(string1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ea0d5e11-9aa2-4323-8b33-16a07d36b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_array = trained.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c5e85be0-2dd9-4ca1-b032-4755ace379cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = countvectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b8bbdd02-11e4-46ea-9097-d1e45371a62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['am', 'are', 'hi', 'how', 'well', 'you'], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bfa2a178-d8ef-4f48-9762-cf87cbe9b59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "94769e49-979f-4a70-b431-2ffbc153722f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 2), ('hi', 1), ('how', 1), ('are', 1), ('you', 1), ('i', 1), ('am', 1), ('well', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Import Counter\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(string2)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [token.lower() for token in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fb9d439f-b4d5-413b-9d53-ce89adf97714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hi', 1), ('well', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "english_stops = set(stopwords.words('english'))\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5ec268cc-3f8c-46f9-9710-fa9bd29450ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'computer' found in the dictionary, ID: 2701\n",
      "First document's word frequencies: [(0, 3), (1, 7), (2, 12), (3, 2), (4, 2), (5, 29), (6, 26), (7, 1), (8, 1), (9, 1)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Download required datasets and tokenizer models\n",
    "#nltk.download('reuters')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# Load documents from the Reuters corpus\n",
    "documents = [reuters.raw(fileid) for fileid in reuters.fileids()]\n",
    "\n",
    "# Tokenize each document\n",
    "tokenized_documents = [word_tokenize(doc.lower()) for doc in documents]  # Convert to lowercase and tokenize\n",
    "\n",
    "# Create a Gensim dictionary from the tokenized documents\n",
    "dictionary = Dictionary(tokenized_documents)\n",
    "\n",
    "# Optional: Find a specific word's ID and print it (e.g., \"computer\")\n",
    "computer_id = dictionary.token2id.get(\"computer\", None)\n",
    "if computer_id is not None:\n",
    "    print(\"Word 'computer' found in the dictionary, ID:\", computer_id)\n",
    "else:\n",
    "    print(\"Word 'computer' not found in the dictionary.\")\n",
    "\n",
    "# Create a corpus in the Bag-of-Words format\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_documents]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the first document\n",
    "if corpus:\n",
    "    print(\"First document's word frequencies:\", corpus[0][:10])\n",
    "else:\n",
    "    print(\"No documents are available in the corpus.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c646f3e5-c938-4150-abc8-15882d119d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
